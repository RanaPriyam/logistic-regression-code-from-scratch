{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,m):\n",
    "    l=len(x)\n",
    "    Pred=np.zeros(l,int)\n",
    "    for i in range(l):\n",
    "        if (m*x[i]).sum()>0:\n",
    "            Pred[i]=1\n",
    "        else:\n",
    "            Pred[i]=0\n",
    "    return Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can avoid this problem by using different cases for positive and negative gamma:\n",
    "\n",
    "# def sigmoid(gamma):\n",
    "#   if gamma < 0:\n",
    "#     return 1 - 1/(1 + math.exp(gamma))\n",
    "#   else:\n",
    "#     return 1/(1 + math.exp(-gamma))\n",
    "# The math range error is likely because your gamma argument is a large negative value, so you are calling exp() with a large positive value. \n",
    "# It is very easy to exceed your floating point range that way\n",
    "def step_GD(x,y,m,a,ite):\n",
    "    import math\n",
    "    l=len(x)\n",
    "    slope=np.zeros(31)\n",
    "    for i in range(31):\n",
    "        for j in range(l):\n",
    "            gamma=(m*x[j]).sum()\n",
    "            if gamma < 0:\n",
    "                h=1 - 1/(1 + math.exp(gamma))\n",
    "            else:\n",
    "                h=1/(1 + math.exp(-gamma))\n",
    "            slope[i]+=(-1/l)*(y[j]-h)*x[j][i]\n",
    "        m[i]-=a*slope[i]\n",
    "    return m\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,m,a,ite):\n",
    "    for i in range(ite):\n",
    "        m=step_GD(x,y,m,a,ite)\n",
    "    return m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1\n",
      " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 1 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0\n",
      "  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "cancer=datasets.load_breast_cancer()\n",
    "temp=cancer.data\n",
    "df=pd.DataFrame(temp)\n",
    "df[30]=1\n",
    "data=df.values\n",
    "result=cancer.target\n",
    "from sklearn import model_selection\n",
    "X_train,X_test,Y_train,Y_test=model_selection.train_test_split(data,result,random_state=0)\n",
    "m=np.zeros(31)\n",
    "a=0.0001\n",
    "ite=1000\n",
    "m=gradient_descent(X_train,Y_train,m,a,ite)\n",
    "Y_test_pred_me=predict(X_test,m)\n",
    "print(Y_test_pred_me)\n",
    "print(Y_test_pred_me-Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression()\n",
    "clf.fit(X_train[:,:30],Y_train)\n",
    "Y_test_pred=clf.predict(X_test[:,:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95        53\n",
      "           1       0.99      0.94      0.97        90\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       143\n",
      "   macro avg       0.95      0.96      0.96       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n",
      "\n",
      "\n",
      "my implementation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91        53\n",
      "           1       0.94      0.97      0.95        90\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       143\n",
      "   macro avg       0.94      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparing my implementation's result with sklearn's logistic regression implementation\n",
    "print(\"sklearn:\")\n",
    "print(classification_report(Y_test,Y_test_pred))\n",
    "print(\"\\n\")\n",
    "print(\"my implementation:\")\n",
    "print(classification_report(Y_test,Y_test_pred_me))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
